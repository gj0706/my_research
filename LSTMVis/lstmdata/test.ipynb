{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cgi\n",
    "\n",
    "import os\n",
    "\n",
    "import h5py\n",
    "import resource\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import helper_functions as hf\n",
    "import yaml\n",
    "import data_handler\n",
    "\n",
    "__author__ = 'Hendrik Strobelt'\n",
    "\n",
    "# Class that handles configuration (yaml) file\n",
    "class LSTMDataHandler:\n",
    "    def __init__(self, directory, config):\n",
    "        \"\"\"LSTM data handler\n",
    "\n",
    "        :param directory: base directory for lstm project\n",
    "        :param config: configuration (YAML file content)\n",
    "        :rtype: None\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "        # storage for h5 references and all dicts\n",
    "        self.h5_files = {}\n",
    "        self.dicts_value_id = {}\n",
    "        self.dicts_id_value = {}\n",
    "\n",
    "        # open all h5 files\n",
    "        h5files = {k: v for k, v in config['files'].items() if (v.endswith('.h5') or v.endswith('.hdf5'))}\n",
    "        for key, file_name in h5files.items():\n",
    "            self.h5_files[key] = h5py.File(os.path.join(directory, file_name), 'r')\n",
    "\n",
    "        # load all dict files of format: value<space>id\n",
    "        dict_files = {k: v for k, v in config['files'].items() if (v.endswith('.dict') or v.endswith('.txt'))}\n",
    "        for name, file_name in dict_files.items():\n",
    "            kv = {}\n",
    "            vk = {}\n",
    "            with open(os.path.join(directory, file_name), 'r') as f:\n",
    "                for line in f:\n",
    "                    if len(line) > 0:\n",
    "                        if len(line.split()) == 2:\n",
    "                            k, v = line.split()\n",
    "                            kv[k] = int(v)\n",
    "                            vk[int(v)] = k\n",
    "                        elif line[0] == \" \":\n",
    "                            k = \" \"\n",
    "                            v = line.strip()\n",
    "                            kv[k] = int(v)\n",
    "                            vk[int(v)] = k\n",
    "                self.dicts_value_id[name] = kv\n",
    "                self.dicts_id_value[name] = vk\n",
    "\n",
    "        # for caching the matrices(cell states and hidden states in each layer)\n",
    "        self.cached_matrix = {}\n",
    "        self.current = {}\n",
    "\n",
    "        # enrich config with sizes\n",
    "        default_state_file = self.config['states']['file']\n",
    "        for x in self.config['states']['types']:\n",
    "            x['file'] = x.get('file', default_state_file)\n",
    "            x['unsigned'] = x.get('unsigned', False)\n",
    "            x['transform'] = x.get('transform', 'tanh')\n",
    "            cell_states, _ = self.get_cached_matrix(x['transform'], x['file'] + '::' + x['path'])\n",
    "            x['size'] = list(cell_states.shape)\n",
    "\n",
    "        ws = self.config['word_sequence']\n",
    "        self.config['word_sequence']['size'] = list(self.h5_files[ws['file']][ws['path']].shape)\n",
    "        self.config['word_sequence']['dict_size'] = len(self.dicts_id_value[ws['dict_file']])\n",
    "\n",
    "        if 'word_embedding' in self.config:\n",
    "            we = self.config['word_embedding']\n",
    "            self.config['word_embedding']['size'] = list(self.h5_files[we['file']][we['path']].shape)\n",
    "        else:\n",
    "            self.config['word_embedding'] = {'size': [-1, -1]}\n",
    "\n",
    "        # enrich meta section with proper ranges\n",
    "        self.config['index'] = os.path.isdir(os.path.join(directory, 'indexdir'))\n",
    "        if self.config['index']:\n",
    "            self.config['index_dir'] = os.path.join(directory, 'indexdir')\n",
    "\n",
    "        if self.config.get('meta', False):\n",
    "            for _, m_info in self.config['meta'].items():\n",
    "                m_info['type'] = m_info.get('type', 'general')\n",
    "                m_info['index'] = m_info.get('index', 'self')\n",
    "                m_info['vis']['range'] = m_info['vis'].get('range', '0...100')\n",
    "                vis_range = m_info['vis']['range']\n",
    "                if vis_range == 'dict':\n",
    "                    m_info['vis']['range'] = self.dicts_value_id[m_info['dict']].keys()\n",
    "                elif type(vis_range) is str:\n",
    "                    m = re.search(\"([0-9]+)\\.\\.\\.([0-9]+)\", vis_range)\n",
    "                    if m:\n",
    "                        m_info['vis']['range'] = range(int(m.group(1)), int(m.group(2)))\n",
    "        else:\n",
    "            self.config['meta'] = []\n",
    "\n",
    "        if self.config.get('etc'):\n",
    "            self.config['etc']['regex_search'] = self.config['etc'].get('regex_search', False)\n",
    "        else:\n",
    "            self.config['etc'] = {\"regex_search\": False}\n",
    "\n",
    "        self.config['is_searchable'] = self.config['index'] or self.config['etc']['regex_search']\n",
    "\n",
    "        if not ('description' in self.config and self.config['description']):\n",
    "            self.config['description'] = self.config['name']\n",
    "\n",
    "    def get_states(self, pos_array, source, left=10, right=0, cell_selection=None, raw=False, round_values=5,\n",
    "                   data_transform='tanh', activation_threshold=0.3, add_active_cells=False, transpose=False, rle=0):\n",
    "\n",
    "        \"\"\"Get information about states.\n",
    "\n",
    "        :param pos_array: array of positions\n",
    "        :param source: source path in states.h5 file\n",
    "        :param left: positions to the left\n",
    "        :param right: positions to the right\n",
    "        :param cell_selection: selection of cells (None if all cells)\n",
    "        :param raw: deliver the states submatrix as numpy array (default:false)\n",
    "        :param round_values: if not raw then round to round_values digits\n",
    "        :param data_transform: data transformation (default: tanh) -- options: raw, tanh, tanhabs\n",
    "        :param activation_threshold: activation threshold for when a cell is considered to be active(default: 0.3)\n",
    "        :param add_active_cells: add active cell count for each position (False)\n",
    "        :param transpose: transpose states sub-matrix and active cell matrix (False)\n",
    "        :return: [ ...{left: position left, right: position right, pos: requestes pos, data: states matrix},...],[sum_active]\n",
    "        :rtype: (list, list)\n",
    "        \"\"\"\n",
    "\n",
    "        if cell_selection is None:\n",
    "            cell_selection = []\n",
    "\n",
    "        cell_states, data_transformed = self.get_cached_matrix(data_transform, source)\n",
    "        # cell_states = self.h5_files[self.config['states']['file']][source]\n",
    "\n",
    "        res = []\n",
    "        sum_active = []\n",
    "        for pos in pos_array:\n",
    "            left_pos = pos - min(left, pos)\n",
    "            right_pos = min(len(cell_states), pos + 1 + right)\n",
    "\n",
    "            if len(cell_selection) == 0:\n",
    "                cs = cell_states[left_pos:right_pos]\n",
    "            else:\n",
    "                cs = cell_states[left_pos:right_pos, cell_selection]\n",
    "\n",
    "            if not data_transformed:\n",
    "                if data_transform == 'tanh':\n",
    "                    np.tanh(cs, cs)\n",
    "                if data_transform == 'tanh_abs':\n",
    "                    np.tanh(cs, cs)\n",
    "                    np.abs(cs, cs)\n",
    "\n",
    "            sub_res = {\n",
    "                'pos': pos,\n",
    "                'left': left_pos,\n",
    "                'right': right_pos - 1\n",
    "            }\n",
    "\n",
    "            if rle > 0:\n",
    "                cs_t = np.transpose(np.copy(cs))\n",
    "                disc = np.copy(cs_t)\n",
    "                cs_t[cs_t < activation_threshold] = 0\n",
    "                hf.threshold_discrete(disc, activation_threshold, 0, 1)\n",
    "\n",
    "                for i in range(0, len(disc)):\n",
    "                    state = disc[i]\n",
    "                    lengths, pos, values = hf.rle(state)\n",
    "                    offset = int(1 - values[0])\n",
    "                    lengths_1 = lengths[offset::2]\n",
    "                    pos_1 = pos[offset::2]\n",
    "                    del_pos = np.argwhere(lengths_1 <= rle)\n",
    "                    for p in del_pos:\n",
    "                        cs_t[i, pos_1[p]:pos_1[p] + lengths_1[p]] = 0\n",
    "                sub_res['data'] = cs_t if raw else [[round(y, round_values) for y in x] for x in cs_t.tolist()]\n",
    "\n",
    "            else:\n",
    "                if transpose:\n",
    "                    sub_res['data'] = np.transpose(cs) if raw else [[round(y, round_values) for y in x] for x in\n",
    "                                                                    np.transpose(cs).tolist()]\n",
    "                else:\n",
    "                    sub_res['data'] = cs if raw else [[round(y, round_values) for y in x] for x in cs.tolist()]\n",
    "\n",
    "            # add count of active cells -- !!! cs will be destroyed here !!!\n",
    "            if add_active_cells:\n",
    "                activation_threshold_corrected = activation_threshold\n",
    "                # already tanh applied if necessary\n",
    "\n",
    "                a = cs\n",
    "                hf.threshold_discrete(a, activation_threshold_corrected, 0, 1)\n",
    "\n",
    "                sum_active.append(np.sum(a, axis=1).tolist())\n",
    "\n",
    "            del cs\n",
    "            res.append(sub_res)\n",
    "\n",
    "        return res, sum_active\n",
    "\n",
    "    def get_words(self, pos_array, left=10, right=0, raw=False, round_values=5, add_embeddings=False):\n",
    "        ws = self.config['word_sequence']\n",
    "        word_sequence = self.h5_files[ws['file']][ws['path']]\n",
    "        embeddings = []\n",
    "        has_embedding = 'file' in self.config['word_embedding']\n",
    "        if add_embeddings and has_embedding:\n",
    "            we = self.config['word_embedding']\n",
    "            embeddings = self.h5_files[we['file']][we['path']]\n",
    "\n",
    "        res = []\n",
    "        cluster = []\n",
    "        for pos in pos_array:\n",
    "            left_pos = pos - min(left, pos)\n",
    "            right_pos = min(len(word_sequence), pos + 1 + right)\n",
    "\n",
    "            word_ids = word_sequence[left_pos:right_pos]\n",
    "\n",
    "            words = []\n",
    "            if 'dict_file' in ws:\n",
    "                mapper = self.dicts_id_value[ws['dict_file']]\n",
    "                words = [mapper[x] for x in word_ids.tolist()]\n",
    "            sub_res = {\n",
    "                'pos': pos,\n",
    "                'word_ids': word_ids.tolist(),\n",
    "                'words': words,\n",
    "                'left': left_pos,\n",
    "                'right': right_pos - 1\n",
    "            }\n",
    "\n",
    "            if add_embeddings and has_embedding:\n",
    "                emb = embeddings[left_pos:right_pos]\n",
    "                sub_res['embeddings'] = emb if raw else [[round(y, round_values) for y in x] for x in emb.tolist()]\n",
    "\n",
    "            res.append(sub_res)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def get_meta(self, name, pos_array, left=10, right=0):\n",
    "        meta = self.config['meta']\n",
    "        if name not in meta:\n",
    "            return []\n",
    "\n",
    "        meta_data_info = meta[name]\n",
    "\n",
    "        if meta_data_info['type'] == 'general':\n",
    "            return self._get_meta_general(meta_data_info, pos_array, left, right)\n",
    "        elif meta_data_info['type'] == 'wordvec':\n",
    "            return self._get_meta_wordvec(meta_data_info, pos_array, left, right)\n",
    "\n",
    "        return []\n",
    "\n",
    "    def _get_meta_wordvec(self, meta_data_info, pos_array, left, right):\n",
    "        word_indices = self.h5_files[meta_data_info['file']][meta_data_info['path']]\n",
    "        max_length = len(word_indices)\n",
    "\n",
    "        has_weights = 'weights_path' in meta_data_info\n",
    "        word_weights = self.h5_files[meta_data_info['file']][meta_data_info['weights_path']] \\\n",
    "            if has_weights else []\n",
    "\n",
    "        has_dict = 'dict' in meta_data_info\n",
    "        word_dict = self.dicts_id_value[meta_data_info['dict']] if has_dict else []\n",
    "\n",
    "        res_indices = []\n",
    "        res_weights = []\n",
    "        res_words = []\n",
    "\n",
    "        for pos in pos_array:\n",
    "            left_pos = pos - min(left, pos)\n",
    "            right_pos = min(max_length, pos + 1 + right)\n",
    "\n",
    "            wi = word_indices[left_pos:right_pos].tolist()\n",
    "            res_indices.append(wi)\n",
    "\n",
    "            if has_dict:\n",
    "                res_words.append([[word_dict[wi] for wi in row] for row in wi])\n",
    "\n",
    "            if has_weights:\n",
    "                weights = word_weights[left_pos:right_pos].tolist()\n",
    "                res_weights.append(weights)\n",
    "\n",
    "        return {'word_ids': res_indices, 'words': res_words, 'weights': res_weights}\n",
    "\n",
    "    def _get_meta_general(self, meta_data_info, pos_array, left, right):\n",
    "        meta_data = self.h5_files[meta_data_info['file']][meta_data_info['path']]\n",
    "        meta_index = meta_data_info.get('index')\n",
    "        if not meta_index:\n",
    "            meta_index = 'self'\n",
    "        res = []\n",
    "        if meta_index == 'self':  # if meta info is related to global coordinates\n",
    "            for pos in pos_array:\n",
    "                left_pos = pos - min(left, pos)\n",
    "                right_pos = min(len(meta_data), pos + 1 + right)\n",
    "\n",
    "                res.append(meta_data[left_pos:right_pos].tolist())\n",
    "        else:  # if meta info is a based on indices from global coordinates (like word index)\n",
    "            position_data = self.h5_files[self.config[meta_index]['file']][self.config[meta_index]['path']]\n",
    "            for pos in pos_array:\n",
    "                left_pos = pos - min(left, pos)\n",
    "                right_pos = min(len(position_data), pos + 1 + right)\n",
    "                meta_indices = position_data[left_pos:right_pos].tolist()\n",
    "                res.append([meta_data[ind].tolist() for ind in meta_indices])\n",
    "\n",
    "        # if there is a dict:\n",
    "        if 'dict' in meta_data_info:\n",
    "            mapper = self.dicts_id_value[meta_data_info['dict']]\n",
    "            res = [[mapper[y] for y in x] for x in res]\n",
    "        return res\n",
    "\n",
    "    def get_dimensions(self, pos_array, source, left, right, dimensions, round_values=5, data_transform='tanh',\n",
    "                       cells=None, activation_threshold=.3, rle=0):\n",
    "        \"\"\" selective information for a sequence\n",
    "\n",
    "        :param rle: filter length\n",
    "        :param pos_array: list of positions\n",
    "        :param source: path in states.h5\n",
    "        :param left: positions left from pos\n",
    "        :param right: positions right from pos\n",
    "        :param dimensions: list of data dimensions to return\n",
    "        :param round_values: round values to x digits\n",
    "        :param data_transform: see :func:`get_alignment`\n",
    "        :param cells: selection of cells\n",
    "        :param activation_threshold: threshold for activation\n",
    "        :return: object for all dimensions\n",
    "        \"\"\"\n",
    "        if cells is None:\n",
    "            cells = []\n",
    "        res = {}\n",
    "        states = None\n",
    "        words_and_embedding = None\n",
    "\n",
    "        for dim in dimensions:\n",
    "            if dim == 'states':\n",
    "                res[dim], cell_active = self.get_states(pos_array, source, left, right,\n",
    "                                                        round_values=round_values,\n",
    "                                                        data_transform=data_transform,\n",
    "                                                        cell_selection=cells,\n",
    "                                                        activation_threshold=activation_threshold,\n",
    "                                                        add_active_cells=('cell_count' in dimensions),\n",
    "                                                        transpose=True, rle=rle)\n",
    "                if 'cell_count' in dimensions:\n",
    "                    res['cell_count'] = cell_active\n",
    "            elif dim == 'words':\n",
    "                res[dim] = self.get_words(pos_array, left, right)\n",
    "            elif dim.startswith('meta_'):\n",
    "                res[dim] = self.get_meta(dim[5:], pos_array, left, right)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def query_similar_activations(self, cells, source, activation_threshold=.3,\n",
    "                                  data_transform='tanh', add_histograms=False, phrase_length=0,\n",
    "                                  query_mode='fast', constrain_left=False, constrain_right=False, no_of_results=50):\n",
    "        \"\"\" search for the longest sequences given the activation threshold and a set of cells\n",
    "\n",
    "        :param cells: the cells\n",
    "        :param source: path in states.h5\n",
    "        :param activation_threshold: threshold\n",
    "        :param data_transform: applied data transformation (tanh, tanhabs, raw)\n",
    "        :return: list of (position, variance of no. active cells, length of longest activation of all cells)\n",
    "        \"\"\"\n",
    "        cell_states, data_transformed = self.get_cached_matrix(data_transform, source)\n",
    "\n",
    "        # print 'before cs:', '{:,}'.format(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n",
    "\n",
    "        activation_threshold_corrected = activation_threshold\n",
    "        if not data_transformed:\n",
    "            activation_threshold_corrected = np.arctanh(activation_threshold)\n",
    "\n",
    "        cut_off = 2\n",
    "\n",
    "        # print 'out cs 1:', '{:,}'.format(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n",
    "\n",
    "        if query_mode == \"fast\":\n",
    "            num_of_cells_per_sum = 5  # how many cells are evaluated per batch\n",
    "            maximal_length = int(5e5)  # only consider the first 500,000 time steps\n",
    "            num_candidates = 1000\n",
    "        else:  # all time steps but still be memory efficient\n",
    "            maximal_length = cell_states.shape[0]\n",
    "            num_of_cells_per_sum = int(np.floor(5e6 / maximal_length))\n",
    "            num_of_cells_per_sum = 1 if num_of_cells_per_sum == 0 else num_of_cells_per_sum\n",
    "            num_candidates = 10000\n",
    "\n",
    "        # print 'num_cells', num_of_cells_per_sum\n",
    "\n",
    "        cs_cand = None\n",
    "        # start = time.time()\n",
    "        no_slices = int(np.ceil(len(cells) * 1. / num_of_cells_per_sum))\n",
    "        for c in range(0, no_slices):\n",
    "            cell_range = cells[c * num_of_cells_per_sum:min((c + 1) * num_of_cells_per_sum, len(cells))]\n",
    "            c_discrete = cell_states[:maximal_length, cell_range]\n",
    "            hf.threshold_discrete(c_discrete, activation_threshold_corrected, 0, 1)\n",
    "\n",
    "            if num_of_cells_per_sum > 1:\n",
    "                c_batch = np.sum(c_discrete, axis=1)\n",
    "            else:\n",
    "                c_batch = c_discrete\n",
    "            if cs_cand is None:\n",
    "                cs_cand = c_batch\n",
    "            else:\n",
    "                cs_cand = cs_cand + c_batch\n",
    "\n",
    "            del c_discrete, c_batch\n",
    "\n",
    "        test_cell_number = len(cells)\n",
    "        test_discrete = np.copy(cs_cand)\n",
    "        collect_all_candidates = {}\n",
    "        # start = time.time()\n",
    "        while test_cell_number > 0 and len(collect_all_candidates) < num_candidates:\n",
    "            if test_cell_number != len(cells):\n",
    "                test_discrete[test_discrete > test_cell_number] = test_cell_number\n",
    "            length, positions, value = hf.rle(test_discrete)\n",
    "            # positions = np.array(positions)\n",
    "            if phrase_length > 0:\n",
    "                indices = np.argwhere((value == test_cell_number) & (length == phrase_length))\n",
    "            else:\n",
    "                indices = np.argwhere((value == test_cell_number) & (length >= cut_off))\n",
    "\n",
    "            if constrain_left and not constrain_right:\n",
    "\n",
    "                len_pos = set(zip(length[indices].flatten().tolist(), positions[indices].flatten().tolist(),\n",
    "                                  (test_cell_number - value[indices - 1]).flatten().astype(int).tolist()))\n",
    "            elif not constrain_left and constrain_right:\n",
    "\n",
    "                len_pos = set(zip(length[indices].flatten().tolist(), positions[indices].flatten().tolist(),\n",
    "                                  (test_cell_number - value[indices + 1]).flatten().astype(int).tolist()))\n",
    "            elif constrain_left and constrain_right:\n",
    "\n",
    "                len_pos = set(zip(length[indices].flatten().tolist(), positions[indices].flatten().tolist(),\n",
    "                                  (test_cell_number - value[indices + 1] - value[indices - 1]).flatten().astype(\n",
    "                                      int).tolist()))\n",
    "            else:\n",
    "                len_pos = set(zip(length[indices].flatten().tolist(), positions[indices].flatten().tolist(),\n",
    "                                  np.zeros(len(indices)).astype(int).tolist()))\n",
    "\n",
    "            for lp in len_pos:\n",
    "                key = '{0}_{1}'.format(lp[0], lp[1])\n",
    "                llp = collect_all_candidates.get(key, lp)\n",
    "                collect_all_candidates[key] = llp\n",
    "\n",
    "            test_cell_number -= 1\n",
    "\n",
    "        all_candidates = list(collect_all_candidates.values())\n",
    "        all_candidates.sort(key=lambda kk: kk[2], reverse=True)\n",
    "        # for k, v in enumerate(all_candidates):\n",
    "        #     if v[1] < 1000:\n",
    "        #         print 'x', v, k\n",
    "        all_candidates = all_candidates[:num_candidates]\n",
    "        # print 'fff'\n",
    "        # for k, v in enumerate(all_candidates):\n",
    "        #     if v[1] < 1000:\n",
    "        #         print 'x', v, k\n",
    "\n",
    "        cell_count = len(cells)\n",
    "\n",
    "        res = []\n",
    "\n",
    "        max_pos = cell_states.shape[0]\n",
    "\n",
    "        for cand in all_candidates:  # positions where all pivot cells start jointly\n",
    "            ml = cand[0]  # maximal length of _all_ pivot cells on\n",
    "            pos = cand[1]  # position of the pattern\n",
    "\n",
    "            if pos < 1 or pos + ml + 1 > max_pos:\n",
    "                continue\n",
    "            # TODO: find a more elegant solution\n",
    "\n",
    "            cs = np.array(cell_states[pos - 1:pos + ml + 1, :])  # cell values of _all_ cells for the range\n",
    "            hf.threshold_discrete(cs, activation_threshold_corrected, -1, 1)  # discretize\n",
    "\n",
    "            # create pattern mask of form -1 1 1..1 -1 = off on on .. on off\n",
    "            mask = np.ones(ml + 2)\n",
    "            mask[0] = -1 if constrain_left else 0  # ignore if not constraint\n",
    "            mask[ml + 1] = -1 if constrain_right else 0  # ignore if not constraint\n",
    "\n",
    "            cs_sum = np.dot(mask, cs)\n",
    "            test_pattern_length = ml  # defines the length of the relevant pattern\n",
    "            test_pattern_length += 1 if constrain_left else 0\n",
    "            test_pattern_length += 1 if constrain_right else 0\n",
    "\n",
    "            all_active_cells = np.where(cs_sum == test_pattern_length)[0]  # all cells  that are active for range\n",
    "\n",
    "            intersect = np.intersect1d(all_active_cells, cells)  # intersection with selected cells\n",
    "            union = np.union1d(all_active_cells, cells)  # union with selected cells\n",
    "\n",
    "            res.append({'pos': pos,\n",
    "                        'factors': [pos, 0, ml,  # int(value[int(indices[ll2]) + 1])\n",
    "                                    (float(len(intersect)) / float(len(union))),  # Jaccard\n",
    "                                    cell_count - len(intersect), len(union),\n",
    "                                    len(intersect)]})  # how many selected cells are not active\n",
    "\n",
    "        def key(elem):\n",
    "            return -elem['factors'][6], elem['factors'][5], -elem['factors'][\n",
    "                2]  # largest intersection, smallest union, longest phrase\n",
    "\n",
    "        meta = {}\n",
    "        if add_histograms:\n",
    "            meta['fuzzy_length_histogram'] = np.bincount([x['factors'][2] for x in res])\n",
    "            meta['strict_length_histogram'] = np.bincount([x['factors'][2] for x in res if x['factors'][4] == 0])\n",
    "\n",
    "        if phrase_length > 1:\n",
    "            res = [x for x in res if x['factors'][2] == phrase_length]\n",
    "\n",
    "        res.sort(key=key)\n",
    "\n",
    "        final_res = list(res[:no_of_results])\n",
    "\n",
    "        # for elem in res_50:\n",
    "        #     print elem, cell_count, -1. * (cell_count - elem[4]) / float(elem[3] + cell_count)\n",
    "        # print(constrain_left, constrain_right)\n",
    "        del res\n",
    "        # print 'out cs 2:', '{:,}'.format(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n",
    "\n",
    "        return final_res, meta\n",
    "\n",
    "    def get_cached_matrix(self, data_transform, source, full_matrix=False):\n",
    "        \"\"\" request the cached full state matrix or a reference to it\n",
    "\n",
    "        :param data_transform: 'tanh' (values: 'tanh', 'raw')\n",
    "        :param source: path in states.h5\n",
    "        :param full_matrix: requires the full matrix to be loaded\n",
    "        :return: tuple(the matrix [reference], has the matrix been data_transformed)\n",
    "        :rtype: (matrix, bool)\n",
    "        \"\"\"\n",
    "\n",
    "        source_file = source.split('::')[0]\n",
    "        source = source.split('::')[1]\n",
    "        cache_id = str(source) + '__' + str(data_transform) + '__' + str(source_file)\n",
    "        # print 'cs:', '{:,}'.format(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n",
    "\n",
    "        if cache_id not in self.cached_matrix and full_matrix:\n",
    "            cell_states = self.h5_files[source_file][source]\n",
    "            if data_transform == 'tanh':\n",
    "                # x = np.zeros(shape=cell_states.shape)\n",
    "                x = np.clip(cell_states, -1, 1)\n",
    "                self.cached_matrix[cache_id] = x\n",
    "\n",
    "        if cache_id in self.cached_matrix:\n",
    "            transformed = True\n",
    "            matrix = self.cached_matrix[cache_id]\n",
    "        else:\n",
    "            transformed = False\n",
    "            matrix = self.h5_files[source_file][source]\n",
    "        # print 'cs:', '{:,}'.format(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n",
    "\n",
    "        return matrix, transformed\n",
    "\n",
    "    def is_valid_source(self, source_id):\n",
    "        split = source_id.split('::')\n",
    "        if len(split) < 2:\n",
    "            return False\n",
    "\n",
    "        source_file = source_id.split('::')[0]\n",
    "        source = source_id.split('::')[1]\n",
    "\n",
    "        b = source in self.h5_files[source_file]\n",
    "\n",
    "        return (source_file in self.h5_files) and \\\n",
    "               (source in self.h5_files[source_file])\n",
    "\n",
    "    def valid_sources(self):\n",
    "        res = []\n",
    "        for x in self.config['states']['types']:\n",
    "            res.append(x['file'] + '::' + x['path'])\n",
    "        return res\n",
    "\n",
    "    def regex_search(self, _query, no_results=20, htmlFormat=False):\n",
    "        ws = self.config['word_sequence']\n",
    "        word_sequence = self.h5_files[ws['file']][ws['path']]\n",
    "\n",
    "        ws_last_pos = len(word_sequence) - 1\n",
    "        pos = 0\n",
    "        hits = []\n",
    "        while pos < ws_last_pos and len(hits) < no_results:\n",
    "            upper_bound = min((pos + 10000), ws_last_pos)\n",
    "            word_ids = word_sequence[pos:upper_bound]\n",
    "\n",
    "            mapper = self.dicts_id_value[ws['dict_file']]\n",
    "            phrase = ''.join([mapper[x][0] for x in word_ids.tolist()])\n",
    "            r = [(m.start() + pos, m.end() + pos, m.group(0)) for m in re.finditer(_query, phrase)]\n",
    "            hits.extend(r)\n",
    "            if upper_bound < ws_last_pos:\n",
    "                pos = upper_bound - 1000\n",
    "            else:\n",
    "                pos = upper_bound\n",
    "        res = []\n",
    "        for h in hits:\n",
    "            min_pos = max(h[0] - 5, 0)\n",
    "            max_pos = min(h[1] + 5, ws_last_pos)\n",
    "            text = ''.join([mapper[x] for x in word_sequence[min_pos:max_pos].tolist()])\n",
    "            res.append({'index': h[0], 'text': cgi.escape(text)})\n",
    "        return res\n",
    "\n",
    "        # if htmlFormat:\n",
    "        #     # results.formatter = HtmlFormatter()\n",
    "        #     return map(lambda y: {'index': y['index'], 'text': y.highlights('content')}\n",
    "        #                , sorted(results, key=lambda k: k['index']))\n",
    "        # else:\n",
    "        #     return map(lambda y: {'index': y['index'], 'text': y['content']}\n",
    "        #                , sorted(results, key=lambda k: k['index']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05childbook\n",
      "<__main__.LSTMDataHandler object at 0x121f56c90>\n",
      "parens\n",
      "<__main__.LSTMDataHandler object at 0x121c632d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaywang/anaconda3/envs/data_science/lib/python3.7/site-packages/ipykernel_launcher.py:14: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "CONFIG_FILE_NAME = 'lstm.yml'\n",
    "data_handlers = {}\n",
    "index_map = {}\n",
    "project_dirs = []\n",
    "# os.walk: iterate all the files and folders under a top directory, returns a 3-element tuple of (root, dirs, files)\n",
    "# root: current directory, dirs: a list of sub directories, files: a list of sub files\n",
    "for root, dirs, files in os.walk('/Users/jaywang/Documents/TTU_study/Fall2019/LSTMVis/data/05childbook/'):\n",
    "    if CONFIG_FILE_NAME in files:\n",
    "        project_dirs.append(os.path.abspath(root)) # os.path.abspath(path) returns the absolute path of 'lstm.yml'\n",
    "\n",
    "i = 0\n",
    "for p_dir in project_dirs:\n",
    "        with open(os.path.join(p_dir, CONFIG_FILE_NAME), 'r') as yf:\n",
    "            config = yaml.load(yf) # config now is a dictionary\n",
    "            dh_id = os.path.split(p_dir)[1] # '05childbook'\n",
    "#             print(dh_id)\n",
    "            data_handlers[dh_id] = LSTMDataHandler(directory=p_dir, config=config)\n",
    "#             print(data_handlers[dh_id])\n",
    "            if data_handlers[dh_id].config['index']:\n",
    "                index_map[dh_id] = data_handlers[dh_id].config['index_dir'] # {'05childbook': '/Users/jaywang/Documents/TTU_study/Fall2019/LSTMVis/data/05childbook/05childbook/indexdir'}\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jaywang/Documents/TTU_study/Fall2019/LSTMVis/data/05childbook/05childbook',\n",
       " '/Users/jaywang/Documents/TTU_study/Fall2019/LSTMVis/data/05childbook/parens']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'05childbook': '/Users/jaywang/Documents/TTU_study/Fall2019/LSTMVis/data/05childbook/05childbook/indexdir'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'05childbook': <__main__.LSTMDataHandler at 0x121f56c90>,\n",
       " 'parens': <__main__.LSTMDataHandler at 0x121c632d0>}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'05childbook': <__main__.LSTMDataHandler at 0x121694990>,\n",
       " 'parens': <__main__.LSTMDataHandler at 0x121689dd0>}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 0.3,\n",
       " 'cells': [2],\n",
       " 'dims': ['states', 'words'],\n",
       " 'left': 3,\n",
       " 'pos': [12],\n",
       " 'project': 'parens',\n",
       " 'right': 3,\n",
       " 'source': 'states::states1',\n",
       " 'transform': 'tanh'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request = {\n",
    "    \"activation\": 0.3,\n",
    "    \"cells\": [\n",
    "      2\n",
    "    ],\n",
    "    \"dims\": [\n",
    "      \"states\",\n",
    "      \"words\"\n",
    "    ],\n",
    "    \"left\": 3,\n",
    "    \"pos\": [\n",
    "      12\n",
    "    ],\n",
    "    \"project\": \"parens\",\n",
    "    \"right\": 3,\n",
    "    \"source\": \"states::states1\",\n",
    "    \"transform\": \"tanh\"\n",
    "  }\n",
    "request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = request['project']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parens'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project in data_handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = data_handlers[project]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LSTMDataHandler at 0x121c632d0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cells = request['cells']\n",
    "cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dh.get_dimensions(\n",
    "            pos_array=request['pos'],\n",
    "            source=request['source'],\n",
    "            left=request['left'],\n",
    "            right=request['right'],\n",
    "            dimensions=request['dims'],\n",
    "            data_transform=request['transform'],\n",
    "            cells=cells,\n",
    "            activation_threshold=request['activation']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'states': [{'pos': 12,\n",
       "   'left': 9,\n",
       "   'right': 15,\n",
       "   'data': [[-0.74336,\n",
       "     -0.74502,\n",
       "     -0.49196,\n",
       "     -0.7894,\n",
       "     -0.74757,\n",
       "     -0.51531,\n",
       "     -0.57777]]}],\n",
       " 'words': [{'pos': 12,\n",
       "   'word_ids': [4, 4, 5, 6, 4, 5, 7],\n",
       "   'words': ['0', '0', '(', ')', '0', '(', '1'],\n",
       "   'left': 9,\n",
       "   'right': 15}]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['cells'] = cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'states': [{'pos': 12,\n",
       "   'left': 9,\n",
       "   'right': 15,\n",
       "   'data': [[-0.74336,\n",
       "     -0.74502,\n",
       "     -0.49196,\n",
       "     -0.7894,\n",
       "     -0.74757,\n",
       "     -0.51531,\n",
       "     -0.57777]]}],\n",
       " 'words': [{'pos': 12,\n",
       "   'word_ids': [4, 4, 5, 6, 4, 5, 7],\n",
       "   'words': ['0', '0', '(', ')', '0', '(', '1'],\n",
       "   'left': 9,\n",
       "   'right': 15}],\n",
       " 'cells': [2]}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = {'request': request, 'results':res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'request': {'activation': 0.3,\n",
       "  'cells': [2],\n",
       "  'dims': ['states', 'words'],\n",
       "  'left': 3,\n",
       "  'pos': [12],\n",
       "  'project': 'parens',\n",
       "  'right': 3,\n",
       "  'source': 'states::states1',\n",
       "  'transform': 'tanh'},\n",
       " 'results': {'states': [{'pos': 12,\n",
       "    'left': 9,\n",
       "    'right': 15,\n",
       "    'data': [[-0.74336,\n",
       "      -0.74502,\n",
       "      -0.49196,\n",
       "      -0.7894,\n",
       "      -0.74757,\n",
       "      -0.51531,\n",
       "      -0.57777]]}],\n",
       "  'words': [{'pos': 12,\n",
       "    'word_ids': [4, 4, 5, 6, 4, 5, 7],\n",
       "    'words': ['0', '0', '(', ')', '0', '(', '1'],\n",
       "    'left': 9,\n",
       "    'right': 15}],\n",
       "  'cells': [2]}}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'05childbook': <__main__.LSTMDataHandler at 0x121f56c90>,\n",
       " 'parens': <__main__.LSTMDataHandler at 0x121c632d0>}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'parens 10k',\n",
       " 'description': 'parens dataset 10k ONLY',\n",
       " 'files': {'states': 'states.hdf5',\n",
       "  'train': 'train.hdf5',\n",
       "  'words': 'train.dict'},\n",
       " 'word_sequence': {'file': 'train',\n",
       "  'path': 'words',\n",
       "  'dict_file': 'words',\n",
       "  'size': [10001],\n",
       "  'dict_size': 10},\n",
       " 'states': {'file': 'states',\n",
       "  'types': [{'type': 'state',\n",
       "    'layer': 1,\n",
       "    'path': 'states1',\n",
       "    'file': 'states',\n",
       "    'unsigned': False,\n",
       "    'transform': 'tanh',\n",
       "    'size': [10000, 200]},\n",
       "   {'type': 'state',\n",
       "    'layer': 2,\n",
       "    'path': 'states2',\n",
       "    'file': 'states',\n",
       "    'unsigned': False,\n",
       "    'transform': 'tanh',\n",
       "    'size': [10000, 200]},\n",
       "   {'type': 'output',\n",
       "    'layer': 2,\n",
       "    'path': 'output2',\n",
       "    'file': 'states',\n",
       "    'unsigned': False,\n",
       "    'transform': 'tanh',\n",
       "    'size': [10000, 200]}]},\n",
       " 'word_embedding': {'size': [-1, -1]},\n",
       " 'index': False,\n",
       " 'meta': [],\n",
       " 'etc': {'regex_search': False},\n",
       " 'is_searchable': False}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_handlers['parens'].config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dive into get_info() function\n",
    "res = []\n",
    "for key, project in data_handlers.items():\n",
    "    # print key\n",
    "    res.append({\n",
    "        'project': key,\n",
    "        'info': project.config\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'project': '05childbook',\n",
       "  'info': {'name': \"Word Model (Children's Books)\",\n",
       "   'description': \"A 1x200 LSTM language model trained on the Gutenberg Children's Book corpus.\",\n",
       "   'files': {'states': 'states.h5',\n",
       "    'train': 'train.h5',\n",
       "    'words': 'words.dict',\n",
       "    'pos': 'pos.h5',\n",
       "    'pos_dict': 'pos.dict',\n",
       "    'ner': 'ner.h5',\n",
       "    'ner_dict': 'ner.dict'},\n",
       "   'word_sequence': {'file': 'train',\n",
       "    'path': 'words',\n",
       "    'dict_file': 'words',\n",
       "    'size': [1271912],\n",
       "    'dict_size': 21688},\n",
       "   'states': {'file': 'states',\n",
       "    'types': [{'type': 'cell',\n",
       "      'layer': 1,\n",
       "      'path': 'states1',\n",
       "      'unsigned': False,\n",
       "      'file': 'states',\n",
       "      'transform': 'tanh',\n",
       "      'size': [1271900, 200]},\n",
       "     {'type': 'hidden',\n",
       "      'layer': 1,\n",
       "      'path': 'output1',\n",
       "      'unsigned': False,\n",
       "      'file': 'states',\n",
       "      'transform': 'tanh',\n",
       "      'size': [1271900, 200]}]},\n",
       "   'meta': {'part_of_speech': {'file': 'pos',\n",
       "     'path': 'pos',\n",
       "     'dict': 'pos_dict',\n",
       "     'vis': {'type': 'discrete',\n",
       "      'range': dict_keys(['ADV', 'NOUN', 'NUM', 'ADP', 'PRON', 'PROPN', 'DET', 'SYM', 'INTJ', 'PART', 'PUNCT', 'VERB', 'X', 'CONJ', 'ADJ'])},\n",
       "     'type': 'general',\n",
       "     'index': 'self'},\n",
       "    'named_entity': {'file': 'ner',\n",
       "     'path': 'ner',\n",
       "     'dict': 'ner_dict',\n",
       "     'vis': {'type': 'discrete',\n",
       "      'range': dict_keys(['ORDINAL', 'LOC', 'PRODUCT', 'NORP', 'WORK_OF_ART', 'LANGUAGE', 'GPE', 'MONEY', 'O', 'PERSON', 'CARDINAL', 'TIME', 'DATE', 'ORG', 'LAW', 'EVENT', 'QUANTITY'])},\n",
       "     'type': 'general',\n",
       "     'index': 'self'}},\n",
       "   'word_embedding': {'size': [-1, -1]},\n",
       "   'index': True,\n",
       "   'index_dir': '/Users/jaywang/Documents/TTU_study/Fall2019/LSTMVis/data/05childbook/05childbook/indexdir',\n",
       "   'etc': {'regex_search': False},\n",
       "   'is_searchable': True}},\n",
       " {'project': 'parens',\n",
       "  'info': {'name': 'parens 10k',\n",
       "   'description': 'parens dataset 10k ONLY',\n",
       "   'files': {'states': 'states.hdf5',\n",
       "    'train': 'train.hdf5',\n",
       "    'words': 'train.dict'},\n",
       "   'word_sequence': {'file': 'train',\n",
       "    'path': 'words',\n",
       "    'dict_file': 'words',\n",
       "    'size': [10001],\n",
       "    'dict_size': 10},\n",
       "   'states': {'file': 'states',\n",
       "    'types': [{'type': 'state',\n",
       "      'layer': 1,\n",
       "      'path': 'states1',\n",
       "      'file': 'states',\n",
       "      'unsigned': False,\n",
       "      'transform': 'tanh',\n",
       "      'size': [10000, 200]},\n",
       "     {'type': 'state',\n",
       "      'layer': 2,\n",
       "      'path': 'states2',\n",
       "      'file': 'states',\n",
       "      'unsigned': False,\n",
       "      'transform': 'tanh',\n",
       "      'size': [10000, 200]},\n",
       "     {'type': 'output',\n",
       "      'layer': 2,\n",
       "      'path': 'output2',\n",
       "      'file': 'states',\n",
       "      'unsigned': False,\n",
       "      'transform': 'tanh',\n",
       "      'size': [10000, 200]}]},\n",
       "   'word_embedding': {'size': [-1, -1]},\n",
       "   'index': False,\n",
       "   'meta': [],\n",
       "   'etc': {'regex_search': False},\n",
       "   'is_searchable': False}}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'project': '05childbook',\n",
       "  'info': {'name': \"Word Model (Children's Books)\",\n",
       "   'description': \"A 1x200 LSTM language model trained on the Gutenberg Children's Book corpus.\",\n",
       "   'files': {'states': 'states.h5',\n",
       "    'train': 'train.h5',\n",
       "    'words': 'words.dict',\n",
       "    'pos': 'pos.h5',\n",
       "    'pos_dict': 'pos.dict',\n",
       "    'ner': 'ner.h5',\n",
       "    'ner_dict': 'ner.dict'},\n",
       "   'word_sequence': {'file': 'train',\n",
       "    'path': 'words',\n",
       "    'dict_file': 'words',\n",
       "    'size': [1271912],\n",
       "    'dict_size': 21688},\n",
       "   'states': {'file': 'states',\n",
       "    'types': [{'type': 'cell',\n",
       "      'layer': 1,\n",
       "      'path': 'states1',\n",
       "      'unsigned': False,\n",
       "      'file': 'states',\n",
       "      'transform': 'tanh',\n",
       "      'size': [1271900, 200]},\n",
       "     {'type': 'hidden',\n",
       "      'layer': 1,\n",
       "      'path': 'output1',\n",
       "      'unsigned': False,\n",
       "      'file': 'states',\n",
       "      'transform': 'tanh',\n",
       "      'size': [1271900, 200]}]},\n",
       "   'meta': {'part_of_speech': {'file': 'pos',\n",
       "     'path': 'pos',\n",
       "     'dict': 'pos_dict',\n",
       "     'vis': {'type': 'discrete',\n",
       "      'range': dict_keys(['ADV', 'NOUN', 'NUM', 'ADP', 'PRON', 'PROPN', 'DET', 'SYM', 'INTJ', 'PART', 'PUNCT', 'VERB', 'X', 'CONJ', 'ADJ'])},\n",
       "     'type': 'general',\n",
       "     'index': 'self'},\n",
       "    'named_entity': {'file': 'ner',\n",
       "     'path': 'ner',\n",
       "     'dict': 'ner_dict',\n",
       "     'vis': {'type': 'discrete',\n",
       "      'range': dict_keys(['ORDINAL', 'LOC', 'PRODUCT', 'NORP', 'WORK_OF_ART', 'LANGUAGE', 'GPE', 'MONEY', 'O', 'PERSON', 'CARDINAL', 'TIME', 'DATE', 'ORG', 'LAW', 'EVENT', 'QUANTITY'])},\n",
       "     'type': 'general',\n",
       "     'index': 'self'}},\n",
       "   'word_embedding': {'size': [-1, -1]},\n",
       "   'index': True,\n",
       "   'index_dir': '/Users/jaywang/Documents/TTU_study/Fall2019/LSTMVis/data/05childbook/05childbook/indexdir',\n",
       "   'etc': {'regex_search': False},\n",
       "   'is_searchable': True}},\n",
       " {'project': 'parens',\n",
       "  'info': {'name': 'parens 10k',\n",
       "   'description': 'parens dataset 10k ONLY',\n",
       "   'files': {'states': 'states.hdf5',\n",
       "    'train': 'train.hdf5',\n",
       "    'words': 'train.dict'},\n",
       "   'word_sequence': {'file': 'train',\n",
       "    'path': 'words',\n",
       "    'dict_file': 'words',\n",
       "    'size': [10001],\n",
       "    'dict_size': 10},\n",
       "   'states': {'file': 'states',\n",
       "    'types': [{'type': 'state',\n",
       "      'layer': 1,\n",
       "      'path': 'states1',\n",
       "      'file': 'states',\n",
       "      'unsigned': False,\n",
       "      'transform': 'tanh',\n",
       "      'size': [10000, 200]},\n",
       "     {'type': 'state',\n",
       "      'layer': 2,\n",
       "      'path': 'states2',\n",
       "      'file': 'states',\n",
       "      'unsigned': False,\n",
       "      'transform': 'tanh',\n",
       "      'size': [10000, 200]},\n",
       "     {'type': 'output',\n",
       "      'layer': 2,\n",
       "      'path': 'output2',\n",
       "      'file': 'states',\n",
       "      'unsigned': False,\n",
       "      'transform': 'tanh',\n",
       "      'size': [10000, 200]}]},\n",
       "   'word_embedding': {'size': [-1, -1]},\n",
       "   'index': False,\n",
       "   'meta': [],\n",
       "   'etc': {'regex_search': False},\n",
       "   'is_searchable': False}}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(res, key=lambda x: x['project'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
